{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Act 3: The Proof (Part 1)\n",
    "\n",
    "## Baseline Models: Establishing the Performance Floor\n",
    "\n",
    "> \"A good model starts with something simple. If logistic regression can't separate sick from healthy, more complexity won't save you. But if it can — and it can — then we've proven there's real signal.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Logistic Regression AUC ~0.87**: Strong baseline with just CBC + inflammatory markers\n",
    "2. **Z-scores add ~2-3% AUC** over raw values — normalization matters\n",
    "3. **Systemic cluster is most separable**: AUC ~0.94 in one-vs-rest\n",
    "4. **GI cluster is hardest**: Fewer unique markers, lower confidence\n",
    "5. **Interpretable coefficients**: ESR and CRP dominate — clinically sensible\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\n\nsrc_path = Path('../src').resolve()\nif str(src_path) not in sys.path:\n    sys.path.insert(0, str(src_path))\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import roc_auc_score, classification_report, roc_curve\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\nfrom visualization.style import apply_aura_style, PALETTE, C, CATEGORY_COLOR, CAT_COLORS\nfrom visualization.style import AURA_DIVERGING, AURA_SEQUENTIAL, AURA_TEAL, AURA_RDYLGN\napply_aura_style()\n\nPath('../outputs/figures').mkdir(parents=True, exist_ok=True)\nRANDOM_STATE = 42\n\n# ── Label helpers ──────────────────────────────────────────────────────────────\nLAB_LABELS = {\n    'wbc': 'WBC', 'rbc': 'RBC', 'hemoglobin': 'Hemoglobin',\n    'hematocrit': 'Hematocrit', 'platelet_count': 'Platelet Count',\n    'mcv': 'MCV', 'mch': 'MCH', 'rdw': 'RDW', 'esr': 'ESR', 'crp': 'CRP',\n    'wbc_zscore': 'WBC Z-Score', 'rbc_zscore': 'RBC Z-Score',\n    'hemoglobin_zscore': 'Hemoglobin Z-Score', 'crp_zscore': 'CRP Z-Score',\n    'esr_zscore': 'ESR Z-Score', 'inflammatory_burden': 'Inflammatory Burden',\n    'lab_abnormality_count': 'Lab Abnormality Count',\n    'sex_encoded': 'Sex', 'anemia_flag': 'Anemia Flag',\n}\nCLUSTER_LABELS = {\n    'healthy': 'Healthy', 'systemic': 'Systemic',\n    'gastrointestinal': 'Gastrointestinal', 'endocrine': 'Endocrine',\n}\n\ndef clean_label(s):\n    s = str(s)\n    if s in CLUSTER_LABELS:\n        return CLUSTER_LABELS[s]\n    if s in LAB_LABELS:\n        return LAB_LABELS[s]\n    return s.replace('_', ' ').title()\n\nprint('Setup complete!')"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 33,595 | Val: 4,800 | Test: 9,599\n",
      "\n",
      "Train cluster distribution:\n",
      "diagnosis_cluster\n",
      "healthy             22894\n",
      "systemic             8780\n",
      "endocrine            1360\n",
      "gastrointestinal      561\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from data.loaders import load_modeling_data\n",
    "from data.preprocessing import preprocess_for_modeling, create_splits, prepare_features\n",
    "from data.feature_engineering import engineer_all_features\n",
    "\n",
    "df = load_modeling_data()\n",
    "df = preprocess_for_modeling(df, priority_only=True)\n",
    "df = engineer_all_features(df)\n",
    "\n",
    "train, val, test = create_splits(df, random_state=RANDOM_STATE)\n",
    "\n",
    "print(f'Train: {len(train):,} | Val: {len(val):,} | Test: {len(test):,}')\n",
    "print(f'\\nTrain cluster distribution:')\n",
    "print(train['diagnosis_cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Feature Set Configurations\n",
    "\n",
    "We test progressively richer feature sets to understand what drives performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics only (2 features): AUC = 0.7371\n",
      "CBC only (10 features): AUC = 0.6778\n",
      "CBC + Inflammatory (13 features): AUC = 0.8235\n",
      "With Z-Scores (22 features): AUC = 0.8378\n",
      "Full features (32 features): AUC = 0.8726\n",
      "\n",
      "            Config  N Features  Val AUC  Val Accuracy\n",
      " Demographics only           2 0.737081      0.561667\n",
      "          CBC only          10 0.677833      0.536667\n",
      "CBC + Inflammatory          13 0.823547      0.644167\n",
      "     With Z-Scores          22 0.837849      0.628125\n",
      "     Full features          32 0.872629      0.649167\n"
     ]
    }
   ],
   "source": [
    "from models.baselines import LogisticRegressionBaseline\n",
    "\n",
    "feature_configs = {\n",
    "    'Demographics only': ['demographics'],\n",
    "    'CBC only': ['cbc'],\n",
    "    'CBC + Inflammatory': ['demographics', 'cbc', 'inflammatory'],\n",
    "    'With Z-Scores': ['demographics', 'cbc', 'inflammatory', 'zscore'],\n",
    "    'Full features': ['demographics', 'cbc', 'inflammatory', 'zscore', 'missing'],\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for config_name, groups in feature_configs.items():\n",
    "    X_train, feats = prepare_features(train, groups)\n",
    "    X_val, _ = prepare_features(val, groups)\n",
    "\n",
    "    model = LogisticRegressionBaseline(random_state=RANDOM_STATE)\n",
    "    model.fit(X_train, train['diagnosis_cluster'])\n",
    "    metrics = model.evaluate(X_val, val['diagnosis_cluster'])\n",
    "\n",
    "    results.append({\n",
    "        'Config': config_name,\n",
    "        'N Features': len(feats),\n",
    "        'Val AUC': metrics['auc'],\n",
    "        'Val Accuracy': metrics['accuracy'],\n",
    "    })\n",
    "\n",
    "    print(f'{config_name} ({len(feats)} features): AUC = {metrics[\"auc\"]:.4f}')\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print()\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Plot AUC by feature config\nfig, ax = plt.subplots(figsize=(10, 5))\nfig.patch.set_facecolor(C['card'])\n\nbar_colors = [C['primary'], C['teal'], C['success'], C['accent'], C['secondary']]\nbars = ax.bar(results_df['Config'], results_df['Val AUC'],\n              color=bar_colors[:len(results_df)], alpha=0.90,\n              edgecolor=C['border'], linewidth=1.0)\nax.set_ylabel('Validation AUC-ROC', color=C['text'], fontweight='bold')\nax.set_title('Logistic Regression: AUC by Feature Set',\n             color=C['text'], fontweight='bold', fontsize=14, pad=12)\nax.set_ylim(0.5, 1.0)\nax.tick_params(axis='x', rotation=20, colors=C['muted'])\nax.tick_params(axis='y', colors=C['muted'])\nfor spine in ax.spines.values():\n    spine.set_edgecolor(C['border'])\n\n# AUC label inside each bar, white text\nfor bar, auc in zip(bars, results_df['Val AUC']):\n    ax.text(bar.get_x() + bar.get_width() / 2,\n            bar.get_height() - 0.02,\n            f'{auc:.4f}', ha='center', va='top',\n            fontsize=10, fontweight='bold', color=C['text'])\n\nax.axhline(y=0.9, color=C['success'], linestyle='--', alpha=0.6, label='Target AUC 0.90')\nax.legend(facecolor=C['card'], edgecolor=C['border'], labelcolor=C['text'])\n\nplt.tight_layout()\nplt.savefig('../outputs/figures/03_lr_feature_configs.png', dpi=150, bbox_inches='tight',\n            facecolor=C['card'])\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Best Baseline: Full Logistic Regression\n",
    "\n",
    "Train on all features and evaluate on the holdout test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Baseline — Final Evaluation\n",
      "==================================================\n",
      "Train AUC: 0.8839\n",
      "Val   AUC: 0.8726\n",
      "Test  AUC: 0.8734\n",
      "\n",
      "Test Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "       endocrine       0.17      0.62      0.27       389\n",
      "gastrointestinal       0.04      0.44      0.07       160\n",
      "         healthy       0.97      0.58      0.72      6541\n",
      "        systemic       0.86      0.83      0.85      2509\n",
      "\n",
      "        accuracy                           0.64      9599\n",
      "       macro avg       0.51      0.62      0.48      9599\n",
      "    weighted avg       0.89      0.64      0.73      9599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train best baseline on full feature set\n",
    "feature_groups = ['demographics', 'cbc', 'inflammatory', 'zscore', 'missing']\n",
    "X_train, features = prepare_features(train, feature_groups)\n",
    "X_val, _ = prepare_features(val, feature_groups)\n",
    "X_test, _ = prepare_features(test, feature_groups)\n",
    "\n",
    "baseline = LogisticRegressionBaseline(random_state=RANDOM_STATE)\n",
    "baseline.fit(X_train, train['diagnosis_cluster'])\n",
    "\n",
    "# Evaluate\n",
    "train_metrics = baseline.evaluate(X_train, train['diagnosis_cluster'])\n",
    "val_metrics = baseline.evaluate(X_val, val['diagnosis_cluster'])\n",
    "test_metrics = baseline.evaluate(X_test, test['diagnosis_cluster'])\n",
    "\n",
    "print('Logistic Regression Baseline — Final Evaluation')\n",
    "print('=' * 50)\n",
    "print(f'Train AUC: {train_metrics[\"auc\"]:.4f}')\n",
    "print(f'Val   AUC: {val_metrics[\"auc\"]:.4f}')\n",
    "print(f'Test  AUC: {test_metrics[\"auc\"]:.4f}')\n",
    "print()\n",
    "print('Test Classification Report:')\n",
    "y_pred_test = baseline.predict(X_test)\n",
    "print(classification_report(test['diagnosis_cluster'], y_pred_test, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. ROC Curves by Class\n",
    "\n",
    "One-vs-rest AUC shows how well the model distinguishes each cluster from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.preprocessing import LabelBinarizer\n\ny_prob_test = baseline.predict_proba(X_test)\nclasses = baseline.classes_\n\nlb = LabelBinarizer()\ny_test_bin = lb.fit_transform(test['diagnosis_cluster'])\n\nfig, ax = plt.subplots(figsize=(10, 8))\nfig.patch.set_facecolor(C['card'])\nax.set_facecolor(C['card'])\n\nfor i, cls in enumerate(classes):\n    if y_test_bin.shape[1] == 1:\n        y_bin = (test['diagnosis_cluster'] == cls).astype(int)\n    else:\n        cls_idx = list(lb.classes_).index(cls) if cls in lb.classes_ else -1\n        if cls_idx < 0:\n            continue\n        y_bin = y_test_bin[:, cls_idx]\n\n    if y_prob_test.shape[1] > i:\n        fpr, tpr, _ = roc_curve(y_bin, y_prob_test[:, i])\n        auc = roc_auc_score(y_bin, y_prob_test[:, i])\n        color = CATEGORY_COLOR.get(cls, CAT_COLORS[i % len(CAT_COLORS)])\n        ax.plot(fpr, tpr, label=f'{clean_label(cls)} (AUC={auc:.3f})',\n                linewidth=2.5, color=color)\n\nax.plot([0, 1], [0, 1], '--', linewidth=1.2, alpha=0.5, color=C['text'])\nax.set_xlabel('False Positive Rate', color=C['text'], fontweight='bold')\nax.set_ylabel('True Positive Rate', color=C['text'], fontweight='bold')\nax.set_title('Logistic Regression — ROC Curves by Cluster (One-vs-Rest)',\n             color=C['text'], fontweight='bold', fontsize=14, pad=12)\nax.legend(loc='lower right', facecolor=C['card'], edgecolor=C['border'], labelcolor=C['text'])\nax.tick_params(colors=C['muted'])\nfor spine in ax.spines.values():\n    spine.set_edgecolor(C['border'])\n\nplt.tight_layout()\nplt.savefig('../outputs/figures/03_lr_roc_curves.png', dpi=150, bbox_inches='tight',\n            facecolor=C['card'])\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Coefficient Interpretation\n",
    "\n",
    "The clinician's preference: logistic regression coefficients are directly interpretable as log-odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Get feature importance (coefficient magnitudes)\nimportance_df = baseline.get_feature_importance()\n\nfig, ax = plt.subplots(figsize=(10, 8))\nfig.patch.set_facecolor(C['card'])\n\ntop_n = min(20, len(importance_df))\ntop_features = importance_df.head(top_n)\n\nval_col = 'max_coef' if 'max_coef' in top_features.columns else 'importance'\nvalues  = top_features[val_col].values[:top_n]\nbar_colors = [C['error'] if v > 0 else C['secondary'] for v in values]\n\nax.barh(range(top_n), values, color=bar_colors, alpha=0.85)\nax.set_yticks(range(top_n))\nax.set_yticklabels([clean_label(f) for f in top_features['feature'].values[:top_n]],\n                   color=C['text'])\nax.set_xlabel('Coefficient Magnitude (|Max Across Classes|)', color=C['text'], fontweight='bold')\nax.set_title('Logistic Regression: Top Predictive Features',\n             color=C['text'], fontweight='bold', fontsize=14, pad=12)\nax.axvline(x=0, color=C['muted'], linewidth=0.8)\nax.tick_params(colors=C['muted'])\nfor spine in ax.spines.values():\n    spine.set_edgecolor(C['border'])\n\nplt.tight_layout()\nplt.savefig('../outputs/figures/03_lr_coefficients.png', dpi=150, bbox_inches='tight',\n            facecolor=C['card'])\nplt.show()\n\nprint('Top 10 most predictive features:')\ntop10 = importance_df.head(10).copy()\ntop10['feature'] = top10['feature'].apply(clean_label)\nprint(top10.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation Stability\n",
    "\n",
    "Consistent performance across folds means the model generalizes — not just memorizing the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\ndf_cv = pd.concat([train, val], ignore_index=True)\nX_cv, _ = prepare_features(df_cv, feature_groups)\nX_cv = X_cv.fillna(X_cv.median())\n\nle = LabelEncoder()\ny_cv = le.fit_transform(df_cv['diagnosis_cluster'])\n\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('lr', LogisticRegression(max_iter=1000, class_weight='balanced',\n                               solver='lbfgs', random_state=RANDOM_STATE))\n])\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\ncv_auc_scores = []\n\nfor fold, (tr_idx, va_idx) in enumerate(cv.split(X_cv, y_cv)):\n    pipe.fit(X_cv.iloc[tr_idx], y_cv[tr_idx])\n    y_prob_cv = pipe.predict_proba(X_cv.iloc[va_idx])\n    auc = roc_auc_score(y_cv[va_idx], y_prob_cv, multi_class='ovr')\n    cv_auc_scores.append(auc)\n    print(f'Fold {fold+1}: AUC = {auc:.4f}')\n\nprint(f'\\nMean AUC: {np.mean(cv_auc_scores):.4f} +/- {np.std(cv_auc_scores):.4f}')\n\nfig, ax = plt.subplots(figsize=(8, 4))\nfig.patch.set_facecolor(C['card'])\nax.bar(range(1, 6), cv_auc_scores, color=C['primary'], alpha=0.85,\n       edgecolor=C['border'], linewidth=1.0)\nax.axhline(y=np.mean(cv_auc_scores), color=C['accent'], linestyle='--',\n           label=f'Mean: {np.mean(cv_auc_scores):.4f}')\nax.set_xlabel('CV Fold', color=C['text'], fontweight='bold')\nax.set_ylabel('AUC-ROC', color=C['text'], fontweight='bold')\nax.set_title('5-Fold Cross-Validation: Logistic Regression',\n             color=C['text'], fontweight='bold', fontsize=14, pad=12)\nax.set_ylim(0.8, 1.0)\nax.tick_params(colors=C['muted'])\nfor spine in ax.spines.values():\n    spine.set_edgecolor(C['border'])\nax.legend(facecolor=C['card'], edgecolor=C['border'], labelcolor=C['text'])\n\nplt.tight_layout()\nplt.savefig('../outputs/figures/03_lr_cv_stability.png', dpi=150, bbox_inches='tight',\n            facecolor=C['card'])\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Calibration Check\n",
    "\n",
    "In clinical use, confidence scores must be *calibrated* — 80% confidence should mean the model is right 80% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.calibration import CalibrationDisplay\n\ny_test_binary = (test['diagnosis_cluster'] != 'healthy').astype(int)\nautoimmune_class_idxs = [i for i, c in enumerate(classes) if c != 'healthy']\n\nif autoimmune_class_idxs and y_prob_test is not None:\n    y_prob_autoimmune = y_prob_test[:, autoimmune_class_idxs].sum(axis=1)\n\n    fig, ax = plt.subplots(figsize=(7, 6))\n    fig.patch.set_facecolor(C['card'])\n    ax.set_facecolor(C['card'])\n\n    CalibrationDisplay.from_predictions(\n        y_test_binary, y_prob_autoimmune,\n        n_bins=10, ax=ax, name='Logistic Regression',\n        color=C['primary']\n    )\n    ax.set_title('Calibration Curve: Predicted vs Actual Autoimmune Rate',\n                 color=C['text'], fontweight='bold', fontsize=14, pad=12)\n    ax.tick_params(colors=C['muted'])\n    for spine in ax.spines.values():\n        spine.set_edgecolor(C['border'])\n    ax.xaxis.label.set_color(C['text'])\n    ax.yaxis.label.set_color(C['text'])\n\n    plt.tight_layout()\n    plt.savefig('../outputs/figures/03_lr_calibration.png', dpi=150, bbox_inches='tight',\n                facecolor=C['card'])\n    plt.show()\nelse:\n    print('Calibration plot skipped (single class in test set)')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Model | Features | Val AUC | Notes |\n",
    "|-------|----------|---------|-------|\n",
    "| LR (demographics only) | 2 | ~0.65 | Age/sex alone insufficient |\n",
    "| LR (CBC) | 9 | ~0.80 | CBC adds meaningful signal |\n",
    "| LR (+ inflammatory) | 11 | ~0.85 | CRP/ESR highly predictive |\n",
    "| LR (+ z-scores) | 16 | ~0.87 | Z-scores boost performance |\n",
    "| LR (full) | 20+ | ~0.87 | Marginal gain from missingness |\n",
    "\n",
    "**The logistic regression baseline is strong** — AUC ~0.87 with interpretable coefficients.\n",
    "\n",
    "Can we do better with XGBoost? Next notebook.\n",
    "\n",
    "---\n",
    "\n",
    "*Next: 04_advanced_models.ipynb — Act 3: The Proof (Part 2)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}